{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando analisis de TED Talks\n",
            "Dataset: ted_talks_en.csv\n",
            "Objetivo: Extraccion de informacion + Modelos ML\n",
            "Modulos cargados correctamente\n"
          ]
        }
      ],
      "source": [
        "# === ANALISIS DE POPULARIDAD DE TED TALKS ===\n",
        "# Aplicacion de Extraccion de Informacion y Comparacion de Modelos ML\n",
        "\n",
        "print(\"Iniciando analisis de TED Talks\")\n",
        "print(\"Dataset: ted_talks_en.csv\")\n",
        "print(\"Objetivo: Extraccion de informacion + Modelos ML\")\n",
        "\n",
        "# Importar la clase principal que controla todo el flujo\n",
        "from modules import TedTalkAnalyzer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Modulos cargados correctamente\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creando instancia del analizador TED Talks...\n",
            "Analizador creado correctamente\n",
            "Metodos disponibles:\n",
            "- setup_environment(): Configurar ambiente\n",
            "- load_data(): Cargar datos\n",
            "- clean_data(): Limpiar datos\n",
            "- process_nlp_features(): Procesar NLP\n",
            "- train_models(): Entrenar modelos ML\n",
            "- create_visualizations(): Crear graficos\n",
            "- run_complete_analysis(): Ejecutar todo automaticamente\n"
          ]
        }
      ],
      "source": [
        "# === CREAR INSTANCIA DEL ANALIZADOR ===\n",
        "\n",
        "print(\"Creando instancia del analizador TED Talks...\")\n",
        "\n",
        "# Crear instancia de la clase principal\n",
        "analyzer = TedTalkAnalyzer()\n",
        "\n",
        "print(\"Analizador creado correctamente\")\n",
        "print(\"Metodos disponibles:\")\n",
        "print(\"- setup_environment(): Configurar ambiente\")\n",
        "print(\"- load_data(): Cargar datos\")\n",
        "print(\"- clean_data(): Limpiar datos\")\n",
        "print(\"- process_nlp_features(): Procesar NLP\")\n",
        "print(\"- train_models(): Entrenar modelos ML\")\n",
        "print(\"- create_visualizations(): Crear graficos\")\n",
        "print(\"- run_complete_analysis(): Ejecutar todo automaticamente\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INICIO: 05:16:05\n",
            "Configurando ambiente y dependencias...\n",
            "Esto puede tomar 2-5 minutos la primera vez\n",
            "==================================================\n",
            "\n",
            "=== CONFIGURANDO AMBIENTE ===\n",
            "=== CONFIGURACION DEL AMBIENTE ===\n",
            "Tiempo estimado: 2-5 minutos\n",
            "\n",
            "PASO 1/3: Instalando 8 paquetes esenciales...\n",
            "  [1/8] Instalando pandas>=1.3.0... OK\n",
            "  [2/8] Instalando numpy>=2.0.0... OK\n",
            "  [3/8] Instalando scikit-learn>=1.0.0... OK\n",
            "  [4/8] Instalando matplotlib>=3.4.0... OK\n",
            "  [5/8] Instalando seaborn>=0.11.0... OK\n",
            "  [6/8] Instalando nltk>=3.7... OK\n",
            "  [7/8] Instalando textblob>=0.17.0... OK\n",
            "  [8/8] Instalando tqdm>=4.64.0... OK\n",
            "\n",
            "Paquetes esenciales: 8/8 instalados\n",
            "\n",
            "PASO 2/3: Instalando 3 paquetes opcionales...\n",
            "  [1/3] Instalando plotly>=5.0.0... OK\n",
            "  [2/3] Instalando spacy>=3.4.0... OK\n",
            "  [3/3] Instalando wordcloud>=1.8.0... OK\n",
            "\n",
            "Paquetes opcionales: 3/3 instalados\n",
            "\n",
            "PASO 3/3: Configurando modelos de NLP...\n",
            "  Descargando datos NLTK... OK\n",
            "  Verificando spaCy... No disponible (se usará NLTK)\n",
            "\n",
            "CONFIGURACION COMPLETADA\n",
            "========================================\n",
            "=== DESCARGANDO MODELOS TRANSFORMER ===\n",
            "⚠ Error configurando transformers: No module named 'transformers'\n",
            "✓ GPU disponible: Tesla T4\n",
            "✗ Error configurando ambiente: name 'load_nlp_models' is not defined\n",
            "\n",
            "Tiempo total: 27.0 segundos\n",
            "COMPLETADO: 05:16:32\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# === CONFIGURACION DEL AMBIENTE ===\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"INICIO:\", datetime.now().strftime(\"%H:%M:%S\"))\n",
        "print(\"Configurando ambiente y dependencias...\")\n",
        "print(\"Esto puede tomar 2-5 minutos la primera vez\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Configurar ambiente usando el metodo del analizador\n",
        "start_time = time.time()\n",
        "analyzer.setup_environment()\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed = end_time - start_time\n",
        "print(f\"\\nTiempo total: {elapsed:.1f} segundos\")\n",
        "print(f\"COMPLETADO:\", datetime.now().strftime(\"%H:%M:%S\"))\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando dataset ted_talks_en.csv...\n",
            "\n",
            "=== CARGANDO DATASET: ted_talks_en.csv ===\n",
            "✓ Dataset cargado: 4005 filas x 19 columnas\n",
            "\n",
            "Columnas disponibles:\n",
            " 1. talk_id\n",
            " 2. title\n",
            " 3. speaker_1\n",
            " 4. all_speakers\n",
            " 5. occupations\n",
            " 6. about_speakers\n",
            " 7. views\n",
            " 8. recorded_date\n",
            " 9. published_date\n",
            "10. event\n",
            "11. native_lang\n",
            "12. available_lang\n",
            "13. comments\n",
            "14. duration\n",
            "15. topics\n",
            "16. related_talks\n",
            "17. url\n",
            "18. description\n",
            "19. transcript\n",
            "Dataset cargado exitosamente\n",
            "Filas: 4,005\n",
            "Columnas: 19\n",
            "Memoria utilizada: 81.26 MB\n",
            "\n",
            "Columnas disponibles:\n",
            "  1. talk_id\n",
            "  2. title\n",
            "  3. speaker_1\n",
            "  4. all_speakers\n",
            "  5. occupations\n",
            "  6. about_speakers\n",
            "  7. views\n",
            "  8. recorded_date\n",
            "  9. published_date\n",
            "  10. event\n",
            "  11. native_lang\n",
            "  12. available_lang\n",
            "  13. comments\n",
            "  14. duration\n",
            "  15. topics\n",
            "  16. related_talks\n",
            "  17. url\n",
            "  18. description\n",
            "  19. transcript\n"
          ]
        }
      ],
      "source": [
        "# === CARGA DE DATOS ===\n",
        "\n",
        "print(\"Cargando dataset ted_talks_en.csv...\")\n",
        "\n",
        "# Cargar datos usando el metodo del analizador\n",
        "analyzer.load_data('ted_talks_en.csv')\n",
        "\n",
        "# Mostrar informacion basica\n",
        "if analyzer.df_original is not None:\n",
        "    print(f\"Dataset cargado exitosamente\")\n",
        "    print(f\"Filas: {analyzer.df_original.shape[0]:,}\")\n",
        "    print(f\"Columnas: {analyzer.df_original.shape[1]}\")\n",
        "    print(f\"Memoria utilizada: {analyzer.df_original.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "    # Mostrar primeras columnas\n",
        "    print(\"\\nColumnas disponibles:\")\n",
        "    for i, col in enumerate(analyzer.df_original.columns):\n",
        "        print(f\"  {i+1}. {col}\")\n",
        "else:\n",
        "    print(\"ERROR: No se pudo cargar el dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aplicando limpieza profesional de datos...\n",
            "\n",
            "=== LIMPIANDO DATOS ===\n",
            "Iniciando: Iniciando limpieza profesional de datos\n",
            "Tiempo de inicio: 05:16:33\n",
            "==================================================\n",
            "[05:16:33] Dataset original: 4005 filas x 19 columnas\n",
            "[1/4] (25.0%) Eliminando outliers con método IQR... OK\n",
            "   📊 Analizando distribución de 'views'...\n",
            "   - Q1 (25%): 882,069\n",
            "   - Q3 (75%): 2,133,110\n",
            "   - IQR: 1,251,041\n",
            "   - Límite inferior: -994,492\n",
            "   - Límite superior: 4,009,672\n",
            "   - Outliers identificados: 393 (9.81%)\n",
            "[05:16:33] Dataset después de eliminar outliers: 3612 filas\n",
            "[2/4] (50.0%) Limpiando datos textuales... OK\n",
            "[05:16:33] Procesando columna: title\n",
            "     - Valores vacíos: 0\n",
            "     - Longitud promedio: 38.4 caracteres\n",
            "[05:16:33] Procesando columna: description\n",
            "     - Valores vacíos: 0\n",
            "     - Longitud promedio: 352.8 caracteres\n",
            "[05:16:34] Procesando columna: transcript\n",
            "     - Valores vacíos: 0\n",
            "     - Longitud promedio: 9870.6 caracteres\n",
            "[05:16:36] Procesadas 3 columnas de texto\n",
            "[3/4] (75.0%) Creando categorías de popularidad... OK\n",
            "   📈 Analizando distribución de popularidad...\n",
            "   Umbrales de popularidad:\n",
            "     - Bajo: hasta 695,206 views\n",
            "     - Medio Bajo: hasta 1,111,279 views\n",
            "     - Medio: hasta 1,470,199 views\n",
            "     - Medio Alto: hasta 1,994,938 views\n",
            "     - Alto: hasta 4,006,448 views\n",
            "\n",
            "   Distribución de categorías:\n",
            "     - Bajo: 723 (20.0%)\n",
            "     - Medio Bajo: 722 (20.0%)\n",
            "     - Medio: 722 (20.0%)\n",
            "     - Medio Alto: 722 (20.0%)\n",
            "     - Alto: 723 (20.0%)\n",
            "[4/4] (100.0%) Validando dataset limpio... OK\n",
            "[05:16:37] Dimensiones finales: 3612 filas x 24 columnas\n",
            "[05:16:37] Filas eliminadas: 393 (9.81%)\n",
            "[05:16:37] Puntuación de calidad: 7.85/10\n",
            "\n",
            "==================================================\n",
            "Estado: Limpieza de datos completada\n",
            "Tiempo total: 3.5 segundos\n",
            "Finalizado: 05:16:37\n",
            "Tiempo promedio por paso: 0.9s\n",
            "✓ Datos limpiados correctamente\n",
            "\n",
            "Resultados de la limpieza:\n",
            "  Filas originales: 4,005\n",
            "  Filas despues de limpieza: 3,612\n",
            "  Filas eliminadas: 393 (9.8%)\n",
            "\n",
            "Categorias de popularidad:\n",
            "  Bajo: 723 videos\n",
            "  Medio Bajo: 722 videos\n",
            "  Medio: 722 videos\n",
            "  Medio Alto: 722 videos\n",
            "  Alto: 723 videos\n",
            "\n",
            "Puntuacion de calidad de datos: 7.85/10\n"
          ]
        }
      ],
      "source": [
        "# === LIMPIEZA DE DATOS ===\n",
        "\n",
        "print(\"Aplicando limpieza profesional de datos...\")\n",
        "\n",
        "# Limpiar datos usando el metodo del analizador\n",
        "analyzer.clean_data()\n",
        "\n",
        "# Mostrar resultados de la limpieza\n",
        "if analyzer.df_clean is not None:\n",
        "    original_count = analyzer.df_original.shape[0]\n",
        "    clean_count = analyzer.df_clean.shape[0]\n",
        "    removed_count = original_count - clean_count\n",
        "    \n",
        "    print(f\"\\nResultados de la limpieza:\")\n",
        "    print(f\"  Filas originales: {original_count:,}\")\n",
        "    print(f\"  Filas despues de limpieza: {clean_count:,}\")\n",
        "    print(f\"  Filas eliminadas: {removed_count:,} ({removed_count/original_count*100:.1f}%)\")\n",
        "    \n",
        "    # Mostrar categorias de popularidad creadas\n",
        "    if 'popularity_category' in analyzer.df_clean.columns:\n",
        "        print(\"\\nCategorias de popularidad:\")\n",
        "        categories = analyzer.df_clean['popularity_category'].value_counts().sort_index()\n",
        "        for category, count in categories.items():\n",
        "            print(f\"  {category}: {count:,} videos\")\n",
        "            \n",
        "    # Mostrar calidad de datos\n",
        "    if 'data_cleaning' in analyzer.results:\n",
        "        quality_score = analyzer.results['data_cleaning']['quality_results']['quality_score']\n",
        "        print(f\"\\nPuntuacion de calidad de datos: {quality_score:.2f}/10\")\n",
        "else:\n",
        "    print(\"ERROR: No se pudo limpiar el dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aplicando tecnicas de extraccion de informacion...\n",
            "Procesando: sentimientos, entidades nombradas, caracteristicas textuales\n",
            "\n",
            "=== PROCESANDO CARACTERÍSTICAS NLP ===\n",
            "✗ Error procesando NLP: name 'process_text_features' is not defined\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'TedTalkAnalyzer' object has no attribute 'df_processed'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mprocess_nlp_features(text_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript_clean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Mostrar caracteristicas extraidas\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_processed\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtraccion de informacion completada\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset procesado: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalyzer\u001b[38;5;241m.\u001b[39mdf_processed\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TedTalkAnalyzer' object has no attribute 'df_processed'"
          ]
        }
      ],
      "source": [
        "# === EXTRACCION DE INFORMACION CON NLP ===\n",
        "\n",
        "print(\"Aplicando tecnicas de extraccion de informacion...\")\n",
        "print(\"Procesando: sentimientos, entidades nombradas, caracteristicas textuales\")\n",
        "\n",
        "# Procesar caracteristicas NLP usando el metodo del analizador\n",
        "analyzer.process_nlp_features(text_column='transcript_clean')\n",
        "\n",
        "# Mostrar caracteristicas extraidas\n",
        "if analyzer.df_processed is not None:\n",
        "    print(f\"\\nExtraccion de informacion completada\")\n",
        "    print(f\"Dataset procesado: {analyzer.df_processed.shape}\")\n",
        "\n",
        "    # Identificar caracteristicas NLP creadas\n",
        "    nlp_features = [col for col in analyzer.df_processed.columns if\n",
        "                   col.startswith(('sentiment_', 'text_', 'person_', 'org_', 'gpe_'))]\n",
        "    \n",
        "    print(f\"\\nCaracteristicas NLP extraidas: {len(nlp_features)}\")\n",
        "    print(\"Tipos de informacion extraida:\")\n",
        "    \n",
        "    # Agrupar por tipo\n",
        "    sentiment_features = [f for f in nlp_features if f.startswith('sentiment_')]\n",
        "    text_features = [f for f in nlp_features if f.startswith('text_')]\n",
        "    entity_features = [f for f in nlp_features if f.startswith(('person_', 'org_', 'gpe_'))]\n",
        "    \n",
        "    if sentiment_features:\n",
        "        print(f\"  Analisis de sentimientos: {len(sentiment_features)} caracteristicas\")\n",
        "    if text_features:\n",
        "        print(f\"  Caracteristicas textuales: {len(text_features)} caracteristicas\") \n",
        "    if entity_features:\n",
        "        print(f\"  Entidades nombradas: {len(entity_features)} caracteristicas\")\n",
        "        \n",
        "    # Mostrar estadisticas de muestra procesada\n",
        "    if 'nlp_processing' in analyzer.results:\n",
        "        sample_size = analyzer.results['nlp_processing']['sample_size']\n",
        "        print(f\"\\nMuestra procesada: {sample_size} registros\")\n",
        "else:\n",
        "    print(\"ERROR: No se pudo procesar las caracteristicas NLP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-03T07:03:25.803244Z",
          "start_time": "2025-08-03T07:03:25.143615Z"
        },
        "cell_id": "5abe3fcf07e4410ea49ff1de3fca1b50",
        "deepnote_cell_type": "code",
        "execution_context_id": "d6f386e3-c5a2-41fb-85f0-dac17a4ccd4f",
        "execution_millis": 95512,
        "execution_start": 1754262986754,
        "source_hash": "ecbd79b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando y comparando modelos de Machine Learning...\n",
            "Objetivo: F1-score > 0.78\n",
            "\n",
            "=== ENTRENANDO MODELOS DE MACHINE LEARNING ===\n",
            "✗ Error entrenando modelos: name 'create_ml_pipeline' is not defined\n",
            "ERROR: No se pudieron entrenar los modelos\n"
          ]
        }
      ],
      "source": [
        "# === ENTRENAMIENTO Y COMPARACION DE MODELOS ML ===\n",
        "\n",
        "print(\"Entrenando y comparando modelos de Machine Learning...\")\n",
        "print(\"Objetivo: F1-score > 0.78\")\n",
        "\n",
        "# Entrenar modelos usando el metodo del analizador\n",
        "analyzer.train_models(text_column='transcript_clean', target_column='popularity_numeric')\n",
        "\n",
        "# Mostrar resultados de los modelos\n",
        "if 'machine_learning' in analyzer.results:\n",
        "    ml_results = analyzer.results['machine_learning']['model_results']\n",
        "    classifier = analyzer.results['machine_learning']['classifier']\n",
        "    \n",
        "    print(\"\\nRESULTADOS DE MODELOS:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Mostrar resultados de cada modelo\n",
        "    for model_name, results in ml_results.items():\n",
        "        if results is not None:\n",
        "            print(f\"\\n{model_name}:\")\n",
        "            print(f\"  Accuracy:  {results['accuracy']:.4f}\")\n",
        "            print(f\"  Precision: {results['precision']:.4f}\")\n",
        "            print(f\"  Recall:    {results['recall']:.4f}\")\n",
        "            print(f\"  F1-Score:  {results['f1_score']:.4f}\")\n",
        "            \n",
        "            # Verificar si cumple objetivo\n",
        "            objetivo_cumplido = \"SI\" if results['f1_score'] > 0.78 else \"NO\"\n",
        "            print(f\"  Objetivo F1>0.78: {objetivo_cumplido}\")\n",
        "    \n",
        "    # Identificar mejor modelo\n",
        "    best_model_name, best_model, best_score = classifier.get_best_model()\n",
        "    print(f\"\\nMEJOR MODELO: {best_model_name}\")\n",
        "    print(f\"F1-Score: {best_score:.4f}\")\n",
        "    \n",
        "    if best_score > 0.78:\n",
        "        print(\"Objetivo cumplido! F1-Score > 0.78\")\n",
        "    else:\n",
        "        print(\"Objetivo no cumplido. Considerar mas datos o mejores caracteristicas.\")\n",
        "        \n",
        "    # Guardar el mejor modelo para referencia\n",
        "    analyzer.best_model_name = best_model_name\n",
        "    analyzer.best_f1_score = best_score\n",
        "else:\n",
        "    print(\"ERROR: No se pudieron entrenar los modelos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-02T20:23:05.092879Z",
          "start_time": "2025-08-02T20:23:05.090755Z"
        },
        "cell_id": "e311e55f07eb4774aef2d27d4ba3f105",
        "deepnote_cell_type": "code",
        "execution_context_id": "d6f386e3-c5a2-41fb-85f0-dac17a4ccd4f",
        "execution_millis": 5768,
        "execution_start": 1754263082313,
        "source_hash": "f325302b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generando metricas de rendimiento y visualizaciones...\n",
            "\n",
            "=== CREANDO VISUALIZACIONES ===\n",
            "✗ Error creando visualizaciones: name 'print_summary_statistics' is not defined\n",
            "\n",
            "Visualizaciones creadas exitosamente:\n",
            "\n",
            "Tipos de visualizaciones disponibles:\n",
            "  - Distribucion de datos\n",
            "  - Correlaciones entre variables\n",
            "  - Metricas de modelos ML\n",
            "  - Matrices de confusion\n",
            "  - Comparacion de rendimiento\n",
            "\n",
            "Analisis completo finalizado\n"
          ]
        }
      ],
      "source": [
        "# === METRICAS DE RENDIMIENTO Y VISUALIZACIONES ===\n",
        "\n",
        "print(\"Generando metricas de rendimiento y visualizaciones...\")\n",
        "\n",
        "# Crear visualizaciones usando el metodo del analizador\n",
        "analyzer.create_visualizations()\n",
        "\n",
        "# Mostrar informacion sobre las visualizaciones creadas\n",
        "if 'visualizations' in analyzer.results:\n",
        "    print(\"\\nVisualizaciones creadas exitosamente:\")\n",
        "    \n",
        "    # Si hay un clasificador disponible, mostrar importancia de caracteristicas\n",
        "    if hasattr(analyzer, 'best_model_name') and 'machine_learning' in analyzer.results:\n",
        "        classifier = analyzer.results['machine_learning']['classifier']\n",
        "        \n",
        "        print(f\"\\nImportancia de caracteristicas del mejor modelo ({analyzer.best_model_name}):\")\n",
        "        try:\n",
        "            feature_importance = classifier.get_feature_importance(analyzer.best_model_name, top_n=10)\n",
        "            for i, (feature, importance) in enumerate(feature_importance, 1):\n",
        "                print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  No se pudo obtener importancia de caracteristicas: {e}\")\n",
        "    \n",
        "    print(\"\\nTipos de visualizaciones disponibles:\")\n",
        "    print(\"  - Distribucion de datos\")\n",
        "    print(\"  - Correlaciones entre variables\")\n",
        "    print(\"  - Metricas de modelos ML\")\n",
        "    print(\"  - Matrices de confusion\")\n",
        "    print(\"  - Comparacion de rendimiento\")\n",
        "else:\n",
        "    print(\"ERROR: No se pudieron crear las visualizaciones\")\n",
        "\n",
        "print(\"\\nAnalisis completo finalizado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ba14dbd4adcd427685627c07bc45b6ec",
        "deepnote_cell_type": "code",
        "execution_context_id": "d6f386e3-c5a2-41fb-85f0-dac17a4ccd4f",
        "execution_millis": 578,
        "execution_start": 1754263088133,
        "source_hash": "8f5ea635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RESUMEN FINAL DEL ANALISIS:\n",
            "==================================================\n",
            "\n",
            "============================================================\n",
            "📋 RESUMEN FINAL DEL ANÁLISIS\n",
            "============================================================\n",
            "📊 Datos procesados: 4005 → 3612 filas\n",
            "📈 Calidad de datos: 7.85/10\n",
            "✅ Pasos completados: 2/6\n",
            "⚠ Algunos pasos no se completaron correctamente\n",
            "\n",
            "ESTADO DEL PROYECTO:\n",
            "INCOMPLETO - No se entrenaron modelos\n",
            "\n",
            "ACCESO A RESULTADOS:\n",
            "- analyzer.data_original: Dataset original\n",
            "- analyzer.data_clean: Dataset limpio\n",
            "- analyzer.data_processed: Dataset con caracteristicas NLP\n",
            "- analyzer.results: Diccionario con todos los resultados\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'TedTalkAnalyzer' object has no attribute 'data_original'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- analyzer.results: Diccionario con todos los resultados\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Mostrar tamanos de datos procesados\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_original\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTAMANOS DE DATOS:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Original: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalyzer\u001b[38;5;241m.\u001b[39mdata_original\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TedTalkAnalyzer' object has no attribute 'data_original'"
          ]
        }
      ],
      "source": [
        "# === RESUMEN FINAL Y CONCLUSIONES ===\n",
        "\n",
        "print(\"RESUMEN FINAL DEL ANALISIS:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Usar el metodo de resumen final del analizador\n",
        "analyzer.print_final_summary()\n",
        "\n",
        "# Informacion adicional sobre el estado del proyecto\n",
        "print(\"\\nESTADO DEL PROYECTO:\")\n",
        "if hasattr(analyzer, 'best_f1_score'):\n",
        "    if analyzer.best_f1_score > 0.78:\n",
        "        print(\"EXITOSO - Objetivo de F1 > 0.78 cumplido\")\n",
        "    else:\n",
        "        print(\"REQUIERE MEJORAS - Objetivo no cumplido\")\n",
        "        print(\"Recomendaciones:\")\n",
        "        print(\"  - Aumentar tamano de la muestra\")\n",
        "        print(\"  - Agregar mas caracteristicas NLP\")\n",
        "        print(\"  - Probar diferentes algoritmos\")\n",
        "        print(\"  - Mejorar limpieza de datos\")\n",
        "else:\n",
        "    print(\"INCOMPLETO - No se entrenaron modelos\")\n",
        "\n",
        "print(\"\\nACCESO A RESULTADOS:\")\n",
        "print(\"- analyzer.data_original: Dataset original\")\n",
        "print(\"- analyzer.data_clean: Dataset limpio\")\n",
        "print(\"- analyzer.data_processed: Dataset con caracteristicas NLP\")\n",
        "print(\"- analyzer.results: Diccionario con todos los resultados\")\n",
        "\n",
        "# Mostrar tamanos de datos procesados\n",
        "if analyzer.df_original is not None:\n",
        "    print(f\"\\nTAMANOS DE DATOS:\")\n",
        "    print(f\"  Original: {analyzer.df_original.shape}\")\n",
        "if analyzer.df_clean is not None:\n",
        "    print(f\"  Limpio: {analyzer.df_clean.shape}\")\n",
        "if analyzer.df_processed is not None:\n",
        "    print(f\"  Procesado: {analyzer.df_processed.shape}\")\n",
        "\n",
        "print(\"\\nANALISIS COMPLETADO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === OPCION: EJECUCION AUTOMATICA COMPLETA ===\n",
        "\n",
        "print(\"OPCION ALTERNATIVA: Ejecutar todo el analisis automaticamente\")\n",
        "print(\"Esta opcion ejecuta todos los pasos en una sola celda\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Descomenta las siguientes lineas para ejecutar todo automaticamente:\n",
        "\n",
        "# print(\"Iniciando analisis automatico...\")\n",
        "# analyzer_auto = TedTalkAnalyzer()\n",
        "# results = analyzer_auto.run_complete_analysis('ted_talks_en.csv')\n",
        "# print(\"Analisis automatico completado\")\n",
        "\n",
        "print(\"\\nEsta opcion automatica ejecuta:\")\n",
        "print(\"  1. Configuracion del ambiente\")\n",
        "print(\"  2. Carga de datos\") \n",
        "print(\"  3. Limpieza de datos\")\n",
        "print(\"  4. Extraccion de informacion NLP\")\n",
        "print(\"  5. Entrenamiento de modelos ML\")\n",
        "print(\"  6. Creacion de visualizaciones\")\n",
        "print(\"  7. Resumen final\")\n",
        "\n",
        "print(\"\\nResultado esperado: F1-score > 0.78 en el mejor modelo\")\n",
        "print(\"Tiempo estimado: 5-10 minutos\")\n",
        "print(\"Dataset optimizado para extraccion de informacion de ted_talks_en.csv\")\n",
        "\n",
        "print(\"\\nPara usar esta opcion:\")\n",
        "print(\"1. Descomenta las lineas 7-10\")\n",
        "print(\"2. Ejecuta esta celda\")\n",
        "print(\"3. Los resultados estaran en 'analyzer_auto' y 'results'\")"
      ]
    }
  ],
  "metadata": {
    "deepnote_notebook_id": "9ef24892143a426ca3c75c3ddd0a2b31",
    "kernelspec": {
      "display_name": "cloudspace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
